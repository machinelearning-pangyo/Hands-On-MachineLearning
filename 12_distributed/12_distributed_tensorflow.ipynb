{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -v -p numpy,sklearn,scipy,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***12장 – 분산 텐서플로***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적\n",
    "- 대규모 NN 을 여러 장치에 병렬 / 분산 실행하여 수행 시간을 절약한다.\n",
    "\n",
    "Tensorflow 분산 처리의 장점\n",
    "- 계산 그래프의 여러 장치 / 머신 분할방법 제어 가능\n",
    "- 다양한 방식의 연산 병렬화 및 동기화 가능\n",
    "\n",
    "병렬 수행의 실사례\n",
    "- 신경망 병렬 수행\n",
    "- 모델 세밀 튜닝을 위해 큰 하이퍼파라미터 공간을 탐색\n",
    "- 대규모 NN 의 앙상블 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.1 단일 머신의 다중 장치**\n",
    "\n",
    "- 단일 머신에 GPU 추가\n",
    "- 다중 머신의 경우 네트워크 비용으로 인해 더 비효율적일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.1 설치*\n",
    "\n",
    "- 그래픽 카드 호환성 확인 : https://developer.nvidia.com/cuda-gpus\n",
    "- CUDA : gpu 를 통한 명시적 computing 을 가능하게 하는 library\n",
    "- cuDNN : DNN 을 위한 기초적 GPU 가속 library\n",
    " - activation function, normalize, forward / back propagation, pooling 등\n",
    "- CUDA, cuDNN 바이너리 다운로드시 nvidia 개발자 계정이 필요 (https://developer.nvidia.com/)\n",
    "\n",
    "\n",
    "* 본 실습에서는 gcp 를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 31 07:48:45 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   73C    P0    88W / 149W |     11MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   74C    P0    87W / 149W |     11MiB / 11441MiB |      9%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyenv, anaconda 등 가상 환경을 사용하고 잇다면 적절한 환경으로 활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 2와 3을 모두 지원합니다. 공통 모듈을 임포트하고 맷플롯립 그림이 노트북 안에 포함되도록 설정하고 생성한 그림을 저장하기 위한 함수를 준비합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 2와 파이썬 3 지원\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# 공통\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 그림을 저장할 폴더\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"distributed\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로컬 서버"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello distributed TensorFlow!\")\n",
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello distributed TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpu 초기화 및 정보 로그가 나온다면 성공\n",
    "\n",
    "[I 12:02:07.853 NotebookApp] Adapting to protocol v5.1 for kernel 11e846df-9d53-4d1a-8be6-0622be66099f\n",
    "2018-07-25 12:02:43.034875: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions tha\n",
    "t this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "2018-07-25 12:02:43.140845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read f\n",
    "rom SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2018-07-25 12:02:43.141396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties\n",
    ": \n",
    "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
    "pciBusID: 0000:00:04.0\n",
    "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
    "2018-07-25 12:02:43.222979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read f\n",
    "rom SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2018-07-25 12:02:43.223631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties\n",
    ": \n",
    "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
    "pciBusID: 0000:00:05.0\n",
    "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
    "2018-07-25 12:02:43.223833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0,\n",
    " 1\n",
    "2018-07-25 12:02:43.774320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecu\n",
    "tor with strength 1 edge matrix:\n",
    "2018-07-25 12:02:43.774378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
    "2018-07-25 12:02:43.774403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
    "2018-07-25 12:02:43.774408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
    "2018-07-25 12:02:43.774839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/jo\n",
    "b:local/replica:0/task:0/device:GPU:0 with 10761 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id\n",
    ": 0000:00:04.0, compute capability: 3.7)\n",
    "2018-07-25 12:02:43.957038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/jo\n",
    "b:local/replica:0/task:0/device:GPU:1 with 10761 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id\n",
    ": 0000:00:05.0, compute capability: 3.7)\n",
    "2018-07-25 12:02:44.140339: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCa\n",
    "che for job local -> {0 -> localhost:33667}\n",
    "2018-07-25 12:02:44.141491: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:334] Started server with t\n",
    "arget: grpc://localhost:33667\n",
    "2018-07-25 12:02:44.152483: I tensorflow/core/distributed_runtime/master_session.cc:1150] Start master session 6e81\n",
    "0a0c2f63410d with config: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.2 GPU RAM 관리*\n",
    "\n",
    "- 단일 GPU 에 여러 프로그램 수행시 경우에 따라 OOM 발생할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 별도 GPU 에 각각 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_VISIBLE_DEVICES=0,1 python3 code.py - 복수 device ID 지정시 multi gpu 로 구동되는지 확인\n",
    "#CUDA_VISIBLE_DEVICES=\"\" 의 경우 CPU 로 수행됨\n",
    "\n",
    "# https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "# http://dongjinlee.tistory.com/entry/%EC%84%A0%ED%83%9D%ED%95%9C-GPU%EC%97%90%EB%A7%8C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%ED%95%A0%EB%8B%B9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# do some tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GPU 메모리 일부만을 사용하도록 강제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 31 07:48:48 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   73C    P0    80W / 149W |  10876MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   74C    P0    76W / 149W |  10876MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      3741      C   /usr/bin/python                            10863MiB |\r\n",
      "|    1      3741      C   /usr/bin/python                            10863MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 필요할 때 메모리를 확장 할당하는 옵션 활용\n",
    "- 비활용 메모리 반납에 시간이 소요되므로(memory fragmentation 을 피하기 위해) 효용성이 낮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto();\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.3 장치에 연산 배치하기*\n",
    "\n",
    "dynamic placer\n",
    "- 사용자 지정 배치 규칙에 비해 좋은 효율성을 보이고 있지 않다고 함.\n",
    "- G사 내부용으로 현재 오픈소스화 되어 잇지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) simple placer\n",
    "* computation graph 수행시 device 에 배치되지 않는 node 를 평가할 때 사용\n",
    "\n",
    "\n",
    "분배 규칙\n",
    "- node 가 이미 배치되어 있는 장비는 해당 node를 그대로 둠\n",
    "- 사용자가 node 를 어떤 장치에 할당햇다면 placer 가 node 를 배치\n",
    "- GPU #0 이 기본, GPU 가 소진되면 CPU 로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 배치 로깅\n",
    "\n",
    "- placer 가 node 를 배치할 시점에 메세지를 기록하는 옵션\n",
    "- TODO : 출력되는 log 에 대한 부연 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "\n",
    "c = a * b\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.initializer.run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:\n",
    "0\n",
    "2018-07-30 21:38:01.248833: I tensorflow/core/common_runtime/placer.cc:886] Variable: (VariableV2)/job:l\n",
    "ocalhost/replica:0/task:0/device:CPU:0\n",
    "Variable/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:38:01.248857: I tensorflow/core/common_runtime/placer.cc:886] Variable/Assign: (Assign)/jo\n",
    "b:localhost/replica:0/task:0/device:CPU:0\n",
    "Variable/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:38:01.248876: I tensorflow/core/common_runtime/placer.cc:886] Variable/read: (Identity)/jo\n",
    "b:localhost/replica:0/task:0/device:CPU:0\n",
    "mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-30 21:38:01.248892: I tensorflow/core/common_runtime/placer.cc:886] mul: (Mul)/job:localhost/rep\n",
    "lica:0/task:0/device:GPU:0\n",
    "Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:38:01.248903: I tensorflow/core/common_runtime/placer.cc:886] Const: (Const)/job:localhost\n",
    "/replica:0/task:0/device:CPU:0\n",
    "Variable/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:38:01.248915: I tensorflow/core/common_runtime/placer.cc:886] Variable/initial_value: (Con\n",
    "st)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "Const_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:38:01.248947: I tensorflow/core/common_runtime/placer.cc:886] Const_1: (Const)/job:localho\n",
    "st/replica:0/task:0/device:CPU:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 동적 배치 함수\n",
    "\n",
    "- tf.device 함수 파라미터로 조건 함수를 넣을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def variables_on_cpu(op):\n",
    "    if(op.type == \"Variable\"):\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/gpu:0\"\n",
    "\n",
    "with tf.device(variables_on_cpu):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 연산과 커널\n",
    "\n",
    "- CPU, GPU 장치별로 지원가능한 연산이 구분되어 잇음\n",
    "- 커널 : 장치에 맞는 구현\n",
    "- 예) integer Variable 은 GPU 에서 지원하지 않음 - 효율성 문제로 강제 미지원\n",
    "- 하시 예시의 경우, dtype=tf.float32 로 명시하지 않으면 숫자 notation 을 통해 정수로 임의판단함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot assign a device for operation 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and devices: \n",
      "Assign: CPU \n",
      "Identity: GPU CPU \n",
      "VariableV2: CPU \n",
      "\n",
      "Colocation members and user-requested devices:\n",
      "  Variable (VariableV2) /device:GPU:0\n",
      "  Variable/Assign (Assign) /device:GPU:0\n",
      "  Variable/read (Identity) /device:GPU:0\n",
      "\n",
      "Registered kernels:\n",
      "  device='CPU'\n",
      "  device='GPU'; dtype in [DT_INT64]\n",
      "  device='GPU'; dtype in [DT_DOUBLE]\n",
      "  device='GPU'; dtype in [DT_FLOAT]\n",
      "  device='GPU'; dtype in [DT_HALF]\n",
      "\n",
      "\t [[Node: Variable = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n",
      "\n",
      "Caused by op 'Variable', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-68108cd7465d>\", line 4, in <module>\n",
      "    a = tf.Variable(3)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n",
      "    constraint=constraint)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 396, in _init_from_args\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 73, in variable_op_v2\n",
      "    shared_name=shared_name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1255, in variable_v2\n",
      "    shared_name=shared_name, name=name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and devices: \n",
      "Assign: CPU \n",
      "Identity: GPU CPU \n",
      "VariableV2: CPU \n",
      "\n",
      "Colocation members and user-requested devices:\n",
      "  Variable (VariableV2) /device:GPU:0\n",
      "  Variable/Assign (Assign) /device:GPU:0\n",
      "  Variable/read (Identity) /device:GPU:0\n",
      "\n",
      "Registered kernels:\n",
      "  device='CPU'\n",
      "  device='GPU'; dtype in [DT_INT64]\n",
      "  device='GPU'; dtype in [DT_DOUBLE]\n",
      "  device='GPU'; dtype in [DT_FLOAT]\n",
      "  device='GPU'; dtype in [DT_HALF]\n",
      "\n",
      "\t [[Node: Variable = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    a = tf.Variable(3) \n",
    "    #a = tf.Variable(3, dtype=tf.float32) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        sess.run(a.initializer) # error\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 간접 배치\n",
    "\n",
    "- 해당 커널이 없는 device 에 할당된 연산을, 해당 커널을 가진 임의 device 에 할당하는 옵션\n",
    "- 묵시적 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    a = tf.Variable(3)\n",
    "    \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "session = tf.Session(config=config)\n",
    "session.run(a.initializer) # /cpu:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:39:03.847717: I tensorflow/core/common_runtime/placer.cc:886] Variable: (VariableV2)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "Variable/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:39:03.847749: I tensorflow/core/common_runtime/placer.cc:886] Variable/Assign: (Assign)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "Variable/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-30 21:39:03.847778: I tensorflow/core/common_runtime/placer.cc:886] Variable/read: (Identity)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "Variable/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-30 21:39:03.847799: I tensorflow/core/common_runtime/placer.cc:886] Variable/initial_value: (Const)/job:localhost/replica:0/task:0/device:GPU:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.4 병렬 실행*\n",
    "\n",
    "CPU\n",
    "\n",
    "1) queueing\n",
    "- nn graph 실행시, 먼저 평가할 연산을 찾은 뒤 관련 연산들의 의존도를 측정\n",
    "- 의존성이 전혀 없는 node 들을 할당된 장치의 evaluation queue 에 추가\n",
    "- 하나의 연산이 평가되면 다른 모든 연산의 dependency counter 가 감소\n",
    "- 그 후 dependency counter 가 0 이 되는 연산이 추가로 장치의 evaluation queue 에 추가\n",
    "- 필요한 모든 Node 가 평가되면 output 을 return\n",
    "\n",
    "2) pooling\n",
    "- cpu 의 evaluation queue 에 있는 연산은 inter-op thread pool 로 이동\n",
    "- 각 상황에 맞도록 병렬 처리  \n",
    " - multi-core hardware 를 이용\n",
    " - multithread cpu 커널 \n",
    "  - 여러개의 부분연산으로 분리하여 다른 evaluation queue 에 배치\n",
    "  - 상기 evaluation queue 에 배치된 연산은 (모든 multithread cpu 커널이 공유하는)intra-op thread pool 로 이동\n",
    "\n",
    "- inter / intra-op thread pool 의 thread 수는 옵션으로 조정가능 - default 0 (모든 코어 사용)\n",
    " - 현재 CPU 특정 코어 지정이 불가하기 때문에, 옵션값을 CPU 코어 수 보다 적에 부여해야함\n",
    "\n",
    "GPU\n",
    "- GPU 상의 evaluation queue 연산들은 순서대로 평가됨\n",
    "- 대다수의 연산에 대한 CUDA / cuDNN 기반 multithread GPU 커널 존재\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.5 제어 의존성*\n",
    "\n",
    "- 의존 연산이 모두 수행되엇음에도, 효율을 위해 evaluation 을 가급적 delay 할 경우\n",
    "-- 다량의 메모리 점유, 다수의 external I/O 발생 등\n",
    "- 다른 연산을 병렬 처리 하며 순차적 실행\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.0)\n",
    "b = a + 2.0\n",
    "\n",
    "with tf.control_dependencies([a,b]):\n",
    "    x = tf.constant(3.0)\n",
    "    y = tf.constant(4.0)\n",
    "    \n",
    "z = x + y # z 도 a, b 의 evaluation 을 기다리는 의존성이 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.2 다중 머신의 다중 장치**\n",
    "\n",
    "- task : 하나 이상의 텐서플로 서버로 구성\n",
    "- job : 각기 이름이 부여된 task group\n",
    "- cluster : task 라고 불리는 하나 이상의 텐서플로 서버로 구성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클러스터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# cluster_spec = tf.train.ClusterSpec({\n",
    "#     \"ps\": [\n",
    "#         \"machine-a-host:2221\",  # /job:ps/task:0\n",
    "#     ],\n",
    "#     \"worker\": [\n",
    "#         \"machine-a-host:2222\",  # /job:worker/task:0 \n",
    "#         \"machine-b-host:2222\",  # /job:worker/task:1\n",
    "#     ]})\n",
    "\n",
    "# # 동일 머신에서 여러 task 수행은 가능하나 비추천. 각 GPU RAM 점유를 수동으로 조정해줘야함\n",
    "# task_ps0 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=0)\n",
    "# task_ps1 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=1)\n",
    "# ...\n",
    "\n",
    "# #server.join() # 원격 클러스터를 호출하여 수행된 서버가 종료될 때까지 기다리도록 메인 스레드를 블록(명시적 서버 종료 방법은 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터에 사용할 원격 서버의 ip 기재\n",
    "# 한 장비(ip)에 port 는 겹치면 정상 동작하지 않는다.\n",
    "ps_task0_ip = \"10.138.0.4:3333\"\n",
    "ps_task1_ip = \"10.138.0.4:3334\"\n",
    "worker_task0_ip = \"10.138.0.2:2221\"\n",
    "worker_task1_ip = \"10.138.0.4:2222\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mgpu = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        ps_task0_ip,  # /job:ps/task:0\n",
    "        ps_task1_ip,  # /job:ps/task:1\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        worker_task0_ip,  # /job:worker/task:0\n",
    "        worker_task1_ip,  # /job:worker/task:1\n",
    "    ]}\n",
    ")\n",
    "\n",
    "task_ps0 = tf.train.Server(cluster_mgpu, job_name=\"ps\", task_index=0)\n",
    "task_ps1 = tf.train.Server(cluster_mgpu, job_name=\"ps\", task_index=1)\n",
    "\n",
    "task_worker0 = tf.train.Server(cluster_mgpu, job_name=\"worker\", task_index=0)\n",
    "task_worker1 = tf.train.Server(cluster_mgpu, job_name=\"worker\", task_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.1 세션 열기*\n",
    "\n",
    "- 각 클러스터에서 tf.train.Server 로 클러스터를 띄운 후, 한 머신의 프로세스의 클라이언트에서 다른 모든 서버에 대해 세션을 열 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.2 마스터와 워커 서비스*\n",
    "\n",
    "- Client / Server 는 gRPC 로 통신함\n",
    " - 적절한 통신 포트를 방화벽에서 열어주어야 함\n",
    "\n",
    "- 텐서플로 서버는 기본적으로 마스터, 워커 서비스를 제공함\n",
    " - 마스터 : 클라이언트가 세션을 열고 그래플르 실행할 수 잇게 해줌\n",
    " - 워커 : 하나의 서버에서 graph 실행을 담당하는 RPC 서비스\n",
    "    \n",
    "- 유연성 제공\n",
    "- 1 Client 가 n Server 에 접속하는 각각 session 오픈 가능\n",
    "- task 마다 1 client 실행\n",
    "- 1 client 로 여러 task 제어\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.3 여러 태스크에 연산 할당하기*\n",
    "\n",
    "- job 이름, task 번호, 장치 유형/번호 지정하여 연산 할당 가능\n",
    "- 장치 유형/번호가 지정되지 않으면 해당 task 의 기본 장치 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/job:ps\"):\n",
    "    a = tf.Variable(1, name=\"a\")\n",
    "    d = tf.Variable(3.0, name=\"d\")\n",
    "\n",
    "with tf.device(\"/job:ps/task:1\"):\n",
    "    e = tf.Variable(5.1, name=\"e\")\n",
    "\n",
    "with tf.device(\"/job:worker/task:0/gpu:0\"):\n",
    "    b = a + 2\n",
    "\n",
    "with tf.device(\"/job:worker/task:1/gpu:1\"):\n",
    "    c = a + b\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "    \n",
    "# 해당 원격 텐서플로 서버에 대한 세션을 오픈, c 를 evaluate 하라는 명령을 전달\n",
    "# 해당 외부 장비의 기본장치(GPU) 에 배치 후 수행, 결과 반환\n",
    "#with tf.Session(target=\"grpc://10.138.0.2:2221\", config=config) as sess: \n",
    "with tf.Session(target=\"grpc://\" + worker_task0_ip, config=config) as sess: \n",
    "    sess.run(a.initializer)\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.4 여러 대의 파라미터 서버에 변수를 나누어 분산하기*\n",
    "\n",
    "- 다량의 파라미터가 있는 대규모 모델의 경우, 서버 한 대로 IO가 몰리지 않게 여러 서버에 분산해둠\n",
    "- 별도 명시설정없이 모든 task 에 round-robin 할당해주는 방법 제공\n",
    "- 대체로 파라미터 서버는 파라미터 저장 / 송수신 용도로 사용되고 무거운 연산을 수행하지 않게 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(tf.train.replica_device_setter( # cluster=cluster_spec 처럼 클러스터 명세를 전달해도 ps_tasks 를 찾아서 이용함\n",
    "        ps_tasks=2,\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker\")):\n",
    "    v1 = tf.Variable(1.0, name=\"v1\")  # /job:ps/task:0 (defaults to /cpu:0) 에 할당\n",
    "    v2 = tf.Variable(2.0, name=\"v2\")  # /job:ps/task:1 (defaults to /cpu:0) 에 할당\n",
    "    v3 = tf.Variable(3.0, name=\"v3\")  # /job:ps/task:0 (defaults to /cpu:0) 에 할당\n",
    "    s = v1 + v2            # /job:worker (defaults to task:0/cpu:0) 에 할당\n",
    "    with tf.device(\"/task:1\"):\n",
    "        p1 = 2 * s         # /job:worker/task:1 (defaults to /cpu:0) 에 할당\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            p2 = 3 * s     # /job:worker/task:1/cpu:0 에 할당\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(\"grpc://\" + worker_task0_ip, config=config) as sess:\n",
    "    v1.initializer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.5 리소스 컨테이너를 사용해 여러 세션에서 상태 공유하기*\n",
    "\n",
    "- local session 은 서로의 상태를 공유할 수 없음\n",
    "- distributed session 의 경우 variable 의 상태를 클러스터 내부의 resource container 로 관리함\n",
    " - 특정 클라이언트 세션 한 곳에서 새 변수를 생성하면 동일 클러스터 내의 다른 세션에서도 자동으로 사용 가능\n",
    " - 리소스 컨테이너는 master task 에서 관리 되는듯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "with tf.container(\"scope01\"):\n",
    "    x = tf.Variable(0.0, name=\"x\")\n",
    "    increment_x = tf.assign(x, x+1)\n",
    "\n",
    "with tf.container(\"scope02\"):\n",
    "    x2 = tf.Variable(0.0, name=\"x\")\n",
    "    increment_x2 = tf.assign(x2, x2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#tf.Session.reset(\"grpc://10.138.0.2:2221\", [\"scope01\"])\n",
    "#tf.Session.reset(\"grpc://10.138.0.2:2221\", [\"scope02\"])\n",
    "\n",
    "with tf.Session(\"grpc://\" + worker_task0_ip, config=config) as sess:\n",
    "    with tf.container(\"scope01\"):\n",
    "        sess.run(x.initializer)\n",
    "        sess.run(increment_x)\n",
    "        print(x.eval())\n",
    "    with tf.container(\"scope02\"):\n",
    "        sess.run(x2.initializer)\n",
    "        sess.run(increment_x2)\n",
    "        sess.run(increment_x2)\n",
    "        print(x2.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.6 텐서플로 큐를 사용한 비동기 통신*\n",
    "\n",
    "- 비동기 데이터 교환을 가능하게 함\n",
    " - 데이터 로더 -> 큐 -> 데이터 학습\n",
    "\n",
    "- placeholder 를 사용해 클러스터에 데이터 주입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FIFOQueue\n",
    " - tuple 지원\n",
    "- RandomShuffleQueue\n",
    "- PaddingFIFOQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# tuple\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "training_instance = tf.placeholder(tf.float32, shape=(2))\n",
    "enqueue = q.enqueue([training_instance]) \n",
    "\n",
    "training_instances = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "enqueue_many = q.enqueue_many([training_instances]) \n",
    "\n",
    "with tf.Session(\"grpc://\" + worker_task0_ip, config=config) as sess:\n",
    "    #sess.run(enqueue, feed_dict={training_instance: [1., 2.]})\n",
    "    sess.run(enqueue_many, feed_dict={training_instances: [[1., 2.], [3., 4.]]})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "# tuple\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "dequeue = q.dequeue() \n",
    "\n",
    "batch_size=2\n",
    "dequeue_many = q.dequeue_many(batch_size) \n",
    "\n",
    "with tf.Session(\"grpc://\" + worker_task0_ip, config=config) as sess:\n",
    "    #sess.run(dequeue)\n",
    "    print(sess.run(dequeue_many))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "6.0\n",
      "3.0\n",
      "4.0\n",
      "dequeue 타임 아웃\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[()])\n",
    "v = tf.placeholder(tf.float32)\n",
    "enqueue = q.enqueue([v])\n",
    "dequeue = q.dequeue()\n",
    "output = dequeue + 1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.operation_timeout_in_ms = 1000\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(enqueue, feed_dict={v: 1.0})\n",
    "    sess.run(enqueue, feed_dict={v: 2.0})\n",
    "    sess.run(enqueue, feed_dict={v: 3.0})\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output, feed_dict={dequeue: 5}))\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output))\n",
    "    try:\n",
    "        print(sess.run(output))\n",
    "    except tf.errors.DeadlineExceededError as ex:\n",
    "        print(\"dequeue 타임 아웃\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.7 그래프에서 직접 데이터 로드하기*\n",
    "\n",
    "- 여러번 전송이 일어나서 대규모 환경에서는 비효율적\n",
    " - 파일IO > 클라이언트 > 마스터 태스크 > 데이터를 필요로하는 다른 태스크\n",
    "- 여러 클라이언트가 동시레 한 자원(데이터) 을 사용할때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 데이터를 변수에 프리로드하기\n",
    "- 훈련 데이터를 한번에 로드하여 변수에 할당(메모리 크기에 맞으면)\n",
    " - 클라이언트 > 클러스터 1회 전송\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 그래프에서 직접 훈련 데이터 읽기\n",
    "- 훈련 데이터가 메모리 크기에 안 맞다면 reader 활용\n",
    "- 파일 시스템에서 직접 데이터를 읽음(클라이언트를 통하지 않고 가능)\n",
    "- 지원 형식 : CSV, fixed length binary record, TFRecords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-9.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "더 이상 읽을 파일이 없습니다\n",
      "[array([[ 4.,  5.],\n",
      "       [17., 81.]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[14., 51.],\n",
      "       [ 1., -1.]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[ 1., -1.],\n",
      "       [ 7.,  8.]], dtype=float32), array([0, 0], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "test_csv = open(\"my_test.csv\", \"w\")\n",
    "test_csv.write(\"x1, x2 , target\\n\")\n",
    "test_csv.write(\"1.,, 0\\n\")\n",
    "test_csv.write(\"4., 5. , 1\\n\")\n",
    "test_csv.write(\"7., 8. , 0\\n\")\n",
    "test_csv.close()\n",
    "\n",
    "test_csv2 = open(\"my_test2.csv\", \"w\")\n",
    "test_csv2.write(\"x1, x2 , target\\n\")\n",
    "test_csv2.write(\"1.,, 0\\n\")\n",
    "test_csv2.write(\"14., 51. , 1\\n\")\n",
    "test_csv2.write(\"17., 81. , 0\\n\")\n",
    "test_csv2.close()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test2.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            sess.run(enqueue_instance)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 읽을 파일이 없습니다\")\n",
    "    sess.run(close_instance_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리더 (Reader) - 예전 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 6, 44]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "default1 = tf.constant([5.])\n",
    "default2 = tf.constant([6])\n",
    "default3 = tf.constant([7])\n",
    "dec = tf.decode_csv(tf.constant(\"1.,,44\"),\n",
    "                    record_defaults=[default1, default2, default3])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coord = tf.train.Coordinator()\n",
    "#threads = tf.train.start_queue_runners(coord=coord)\n",
    "#filename_queue = tf.train.string_input_producer([\"test.csv\"])\n",
    "#coord.request_stop()\n",
    "#coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Coordinator 와 QueueRunner 를 사용하는 멀티스레드 리더\n",
    "\n",
    "- 여러 스레드가 여러 리더로 여러 파일을 동시에 read 할 목적\n",
    "- 스레드 구현 및 제어 관리를 신경쓰지 않고 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-10.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 7.,  8.],\n",
      "       [ 1., -1.]], dtype=float32), array([0, 0], dtype=int32)]\n",
      "[array([[17., 81.],\n",
      "       [ 1., -1.]], dtype=float32), array([0, 0], dtype=int32)]\n",
      "[array([[14., 51.],\n",
      "       [ 4.,  5.]], dtype=float32), array([1, 1], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "n_threads = 5\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, [enqueue_instance] * n_threads)\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test2.csv\"})    \n",
    "    sess.run(close_filename_queue)\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1., -1.],\n",
      "       [ 4.,  5.]], dtype=float32), array([0, 1], dtype=int32)]\n",
      "[array([[14., 51.],\n",
      "       [ 1., -1.]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[17., 81.],\n",
      "       [ 7.,  8.]], dtype=float32), array([0, 0], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def read_and_push_instance(filename_queue, instance_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "    features = tf.stack([x1, x2])\n",
    "    enqueue_instance = instance_queue.enqueue([features, target])\n",
    "    return enqueue_instance\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "read_and_enqueue_ops = [read_and_push_instance(filename_queue, instance_queue) for i in range(5)]\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test2.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.4에서 소개된 Data API를 사용하면 손쉽게 데이터를 효율적으로 읽을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0에서 9까지 정수를 세 번 반복한 간단한 데이터셋을 일곱 개씩 배치로 만들어 시작해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "dataset = dataset.repeat(3).batch(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 줄은 0에서 9까지 정수를 담은 데이터셋을 만듭니다. 두 번째 줄은 이 데이터셋의 원소를 세 번 반복하고 일곱 개씩 담은 새로운 데이터셋을 만듭니다. 위에서 볼 수 있듯이 원본 데이터셋에서 여러 변환 메서드를 연결하여 호출하여 적용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음, 데이터셋을 한 번 순회하는 원-샷-이터레이터(one-shot-iterator)를 만들고, 다음 원소를 지칭하는 텐서를 얻기 위해 `get_next()` 메서드를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`next_element`를 반복적으로 평가해서 데이터셋을 순회해 보죠. 원소가 별로 없기 때문에 `OutOfRangeError`가 발생합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[7 8 9 0 1 2 3]\n",
      "[4 5 6 7 8 9 0]\n",
      "[1 2 3 4 5 6 7]\n",
      "[8 9]\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(next_element.eval())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋네요! 잘 작동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "늘 그렇듯이 텐서는 그래프를 실행(`sess.run()`)할 때마다 한 번만 평가된다는 것을 기억하세요. `next_element`에 의존하는 텐서를 여러개 평가하더라도 한 번만 평가됩니다. 또한 `next_element`를 동시에 두 번 실행해도 마찬가지입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3, 4, 5, 6]), array([0, 1, 2, 3, 4, 5, 6])]\n",
      "[array([7, 8, 9, 0, 1, 2, 3]), array([7, 8, 9, 0, 1, 2, 3])]\n",
      "[array([4, 5, 6, 7, 8, 9, 0]), array([4, 5, 6, 7, 8, 9, 0])]\n",
      "[array([1, 2, 3, 4, 5, 6, 7]), array([1, 2, 3, 4, 5, 6, 7])]\n",
      "[array([8, 9]), array([8, 9])]\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([next_element, next_element]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interleave()` 메서드는 강력하지만 처음에는 이해하기 좀 어렵습니다. 예제를 통해 이해하는 것이 가장 좋습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "dataset = dataset.interleave(\n",
    "    lambda v: tf.data.Dataset.from_tensor_slices(v),\n",
    "    cycle_length=3,\n",
    "    block_length=2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,7,8,4,5,2,3,9,0,6,7,4,5,1,2,8,9,6,3,0,1,2,8,9,3,4,5,6,7,완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(next_element.eval(), end=\",\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cycle_length=3`이므로 새로운 데이터셋은 이전 데이터셋에서 세 개의 원소를 추출합니다. 즉 `[0,1,2,3,4,5,6]`, `[7,8,9,0,1,2,3]`, `[4,5,6,7,8,9,0]` 입니다. 그다음 원소마다 하나의 데이터셋을 만들기 위해 람다(lambda) 함수를 호출합니다. `Dataset.from_tensor_slices()`를 사용했기 때문에 각 데이터셋은 차례대로 원소를 반환합니다. 다음 이 세 개의 데이터셋에서 각각 두 개의 아이템(`block_length=2`이므로)을 추출합니다. 세 개의 데이터셋의 아이템이 모두 소진될 때까지 반복됩니다. 즉 0,1 (첫 번째에서), 7,8 (두 번째에서), 4,5 (세 번째에서), 2,3 (첫 번째에서), 9,0 (두 번째에서) 등과 같은 식으로 8,9 (세 번째에서), 6 (첫 번째에서), 3 (두 번째에서), 0 (세 번째에서)까지 진행됩니다. 그다음에 원본 데이터셋에서 다음 번 세 개의 원소를 추출하려고 합니다. 하지만 두 개만 남아 있습니다. `[1,2,3,4,5,6,7]`와 `[8,9]` 입니다. 다시 이 원소로부터 데이터셋을 만들고 이 데이텃세의 아이템이 모두 소진될 때까지 두 개의 아이템을 추출합니다. 1,2 (첫 번째에서), 8,9 (두 번째에서), 3,4 (첫 번째에서), 5,6 (첫 번째에서), 7 (첫 번째에서)가 됩니다. 배열의 길이가 다르기 때문에 마지막에는 교대로 배치되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리더 (Reader) - 새로운 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_tensor_slices()`나 `from_tensor()`를 기반으로 한 원본 데이터셋을 사용하는 대신 리더 데이터셋을 사용할 수 있습니다. 복잡한 일들을 대부분 대신 처리해 줍니다(예를 들면, 스레드):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"my_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 줄을 어떻게 디코드해야 하는지는 알려 주어야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_csv_line(line):\n",
    "    x1, x2, y = tf.decode_csv(\n",
    "        line, record_defaults=[[-1.], [-1.], [-1.]])\n",
    "    X = tf.stack([x1, x2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음, 이 디코딩 함수를 `map()`을 사용하여 데이터셋에 있는 각 원소에 적용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.skip(1).map(decode_csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 원-샷-이터레이터를 만들어 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "X, y = it.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1.] 0.0\n",
      "[4. 5.] 1.0\n",
      "[7. 8.] 0.0\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            X_val, y_val = sess.run([X, y])\n",
    "            print(X_val, y_val)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 텐서플로 클러스터에서 신경망 병렬화하기**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.1 장치마나 하나의 신경망*\n",
    "\n",
    "- 가장 단순함, 직관적\n",
    "- 여러 하이퍼파라미터를 돌려보며 튜닝하는 목적\n",
    "- realtime prediction 전용 환경으로도 좋음(gpu 를 증가시켜 scale-out 가능)\n",
    "\n",
    "- tensorflow serving\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.2 그래프 내 복제와 그래프 간 복제*\n",
    "\n",
    "- 여러 신경망을 분산배치하여 앙상블을 구성할 수 잇음\n",
    "- 모든 신경망들의 prediction 을 모아서 앙상블의 prediction 이 가능\n",
    "- 앙상블 구성방법\n",
    " - 그래프 내 복제 : 구현이 간단함\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 그래프 간 복제 : 구현이 복잡하지만, queue 통신 기반이기 때문에 앙상블의 안정성을 좀 더 보장해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.3 모델 병렬화*\n",
    "\n",
    "- 모델을 여러 부분으로 나누어 각 부분을 다른 장치에서 실행\n",
    "- 신경망 모델 구조에 따라 효율성과 구현 난이도가 갈림\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fully connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Partialy connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-15.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Deep RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.4 데이터 병렬화*\n",
    "\n",
    "- 각 장치에 모델 복제 후 각각 다른 데이터(미니배치) 를 사용\n",
    "- 훈련 후 발생하는 gradient 를 취합 후 모델 파라미터 업데이트\n",
    " - 동기 / 비동기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-17.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 동기\n",
    "\n",
    "- 모든 gradient 가 계산될때까지 대기한 후 평균을 계산\n",
    "- 거의 같은 시점에 모든 복제모델에 파라미터가 업데이트 \n",
    "\n",
    "2) 비동기\n",
    "\n",
    "- gradient 가 계산될 시점에 그때그때 모델 파라미터 업데이트\n",
    "- 훈련학습이 빨리되는 모델이 더 파라미터를 자주 업데이트하게 됨\n",
    "- stale gradient\n",
    " - gradient 가 계산된 후 그 값으로 파라미터가 업데이트 되기 전에 다른 모델이 파라미터를 업데이트 한다면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12-18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 대역폭 포화\n",
    "\n",
    "- gpu ram i/o 시간 > gpu 연산 계산시간(분할된)\n",
    " - 네트워크나 버스로?\n",
    "- 모델이 규모가 작고 데이터 량이 많다면 1 gpu 1 machine 이 더 낫다고함\n",
    "\n",
    "- 대역폭 감소 방안\n",
    " - 적은 수의 머신에 GPU 를 모아서 네트워크 통신 최소화\n",
    " - 여러 대의 파라미터 서버에 파라미터 분산\n",
    " - 모델 파라미터 정밀도 조정 (float32 -> float16)\n",
    " \n",
    "4) 텐서플로 구현 \n",
    "\n",
    "- 그래프 내 복제 / 그래프 간 복제, 동기 / 비동기 등 방법 선택\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 연습문제 해답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ageron/handson-ml/issues/187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 텐서플로 프로그램 수행시 CUDA_ERROR_OUT_OF_MEMORY 오류가 발생하는 원인 및 해결책은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (장치) 연산 할당 / 연산 배치 의 차이점."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GPU 버젼 텐서플로의 기본 배치로 실행하면 모든 연산이 GPU #0 에 배치되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. \"/gpu:0\" 에 할당된 Variable 을 \"/gpu:1\" 상의 연산이 사용 할 수 있는가?\n",
    "\n",
    "    또는 \"/cpu:0\" 상의 연산이 사용 할 수 있는가? 다른 서버 장치에 할당된 연산은?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 같은 장치에 배치된 두 연산이 동시에 수행될 수 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 제어 의존성이 무엇인가? 언제 사용되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 클러스터로 며칠간 DNN 을 훈련한 후 Saver 로 모델을 저장하지 않앗다면 모델은 날아간 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 클러스터에서 몇 개의 DNN 을 각기 다른 hparam 으로 훈련해 보라.\n",
    "   cross validation 또는 validation set 을 통해 가장 좋은 모델 세 개를 선택하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 상기 선택된 모델 세 개로 앙상블을 구성하라. 개별 DNN 보다 성능이 더 나은가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 그래프 간 복제, 비동기 업데이트를 통해 DNN 을 훈련 시켜보라. 동기 업데이트를 사용해 다시 훈련해보라.\n",
    "    성능 및 훈련 소요 시간을 비교해보라.\n",
    "    DNN 수직 분할 후 훈련시켰을 때도 추가로 비교해 보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
